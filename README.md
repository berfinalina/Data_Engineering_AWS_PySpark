# Large Dataset Analysis with PySpark, Databricks, and AWS S3

This project uses PySpark on Databricks and AWS S3 to perform large-scale data analysis, extracting actionable insights from a comprehensive match dataset. The analysis includes performance metrics, team strategies, and impact assessments for data-driven decision-making.

## Project Overview

The project focuses on processing, analyzing, and visualizing complex datasets related to player and match statistics. The objective is to understand player performance, team dynamics, and specific metrics, enabling deeper insights into impactful events.

### Key Features
- **Data Ingestion and Storage**: Utilizes AWS S3 for reliable data storage and Databricks for distributed data processing.
- **Data Transformation and Cleaning**: Applies PySpark to preprocess data, standardize fields, handle missing values, and manage large datasets efficiently.
- **Feature Engineering**: Extracts additional features such as player batting/bowling styles, high-impact plays, and aggregate performance metrics.
- **Exploratory Data Analysis**: Focuses on calculating top-performing players, economic bowlers in initial overs, and the influence of match strategies.

## Technical Stack
- **Python**
- **PySpark**
- **Databricks**
- **AWS S3**
- **Matplotlib & Seaborn** (for data visualization)

## Setup and Installation

### Prerequisites
1. **Python 3.7+** installed locally.
2. **Databricks Account**: Sign up at [Databricks](https://databricks.com/) and set up a workspace.
3. **AWS Account**: For setting up an S3 bucket to store the dataset.

### Steps
1. Clone this repository:
   ```bash
   git clone https://github.com/berfinalina/Data_Engineering_AWS_PySpark.git

